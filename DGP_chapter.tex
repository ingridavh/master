\documentclass[twoside,english]{uiofysmaster}
%\bibliography{references}

\usepackage{float}
\usepackage{scrextend}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{algorithm2e}
\addtokomafont{labelinglabel}{\sffamily}

\begin{document}


%\title{Distributed Gaussian Processes}
%\author{Ingrid Holm}
%\date{September 2017}

%\maketitle

\chapter{Distributed Gaussian Processes}

Distributed Gaussian Processes \cite{deisenroth2015distributed} are a combination of the Bayesian Comittee Machine (BCM) and Product of Experts (PoE). In the first subchapter we review BCM, and in the second DGP. In the third chapter the algorithm for DGP is presented.

\section{Bayesian Comittee Machine}

The Bayesian Comittee Machine \cite{tresp2000bayesian} is based on Bayesian statistics.

\section{Distributed Gaussian Process}

Consider the regression problem $y = f(x) + \epsilon \in \mathbb{R}$, where $x \in \mathbb{R}^D$. The noise is distributed according to a Gaussian with mean zero and variance $\sigma_{\epsilon}^2$ : $\epsilon \sim \mathcal{N}(0, \sigma_{\epsilon}^2)$. We use a set of training data, consisting of $N$ $D$-dimensional vectors and their corresponding target values, namely the sets $\textbf{X} = \{\textbf{x}_i\}_{i=1}^N$ and $\textbf{y} = \{y_i\}_{i=1}^N$. Since a Gaussian Process uses the normal distribution, is it fully spesified by the mean $m$ and covariance function $k$, also called the \textit{kernel}.

To find the best fit, a GP normally minimizes the log-marginal likelihood
\begin{align}
\log p (\textbf{y}|\textbf{X}, \boldsymbol{\theta}) = - \frac{1}{2} (\textbf{y}^T (\textbf{K} + \sigma_{\epsilon}^2 \mathbb{I})^{-1} \textbf{y} + \log |\textbf{K} + \sigma_{\epsilon}^2 \mathbb{I}|).
\end{align}

Here, $\textbf{K} = k(\textbf{X}, \textbf{X}) \in \mathbb{R}^{N \times N}$ is the kernel matrix of the training points, which is a measure of the covariance between points. A typical kernel is the static squared-exponential kernel. All the kernel matrices used are given by
\begin{align}
\textbf{K} &= k(\textbf{X}, \textbf{X}) \in \mathbb{R}^{N \times N},\\
\textbf{k}_* &= k(\textbf{X}, \textbf{x}_*) \in \mathbb{R}^{N}\\
k_{**} &= k(\textbf{x}_*, \textbf{x}_*) \in \mathbb{R},
\end{align}
for a single test point $\textbf{x}_* \in \mathbb{R}^D$.

The rBCM's predictive distribution is
\begin{align}
p(f_*|x_*, \mathcal{D}) = \frac{\prod_{k=1}^Mp_k^{\beta_k} (f_*|x_*, \mathcal{D}^{(k)}) }{p^{-1+ \sum_k \beta_k (f_*|x_*)}},
\end{align}
where the predictive mean and covariance are given by
\begin{align}
\mu_*^{rbcm} &= (\sigma_*^{rbcm})^2 \sum_k \beta_k \sigma_k^{-2} (\textbf{x}_*) \mu_k (\textbf{x}_*),\\
(\sigma_*^{rbcm})^{-2} &= \sum_{k=1}^M \beta_k \sigma_k^{-2} (\textbf{x}_*) + \big(1 - \sum_{k=1}^M \beta_k \big) \sigma_{**}^{-2}.
\end{align}
The posterior distribution for the test point $\textbf{x}_*$ is given by a Gaussian with mean and variance
\begin{align}
\mu (\textbf{x}_*) &= \textbf{k}_*^T (\textbf{K} + \sigma_{\epsilon}^2 \mathbb{I})^{-1} \textbf{y},\\
\sigma^2(\textbf{x}_*) &= k_{**} - \textbf{k}_*^T(\textbf{K} + \sigma_{\epsilon}^2 \mathbb{I})^{-1} \textbf{k}_*.
\end{align}


\begin{algorithm}[H]
 \KwData{Training set $\textbf{X} = \{\textbf{x}_i\}_{i=1}^N$ and $\textbf{y} = \{y_i\}_{i=1}^N$, and test point $\textbf{x}_* \in \mathbb{R}^D$.}
 \KwResult{Approximative distribution of $f_* = f(\textbf{x}_*)$ with mean $\mu_{rbcm}$ and variance $\sigma^2_{rbcm}$.}
 initialization\;
 \For {$i$th expert}{ $gp_{temporary}$ = Fit GP to Training points \;
 Find predicted $\mu_k (\textbf{x}_*)$ and $\sigma_k^2(\textbf{x}_*)$ \; 
 \For {$k$th test point}{Do something}
 \For {$k$th test point}{Do more stuff}
 }
 \caption{Algorithm for using rBCM on a single test point $\textbf{x}_*$.}
\end{algorithm}


\bibliographystyle{plain}
\bibliography{DGP_chapter}

\end{document}


