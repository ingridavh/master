\documentclass[twoside,english]{uiofysmaster}
%\bibliography{references}

\usepackage{array}
\usepackage{booktabs}
\usepackage{float}
\usepackage{scrextend}
\usepackage{amsfonts}
\usepackage{amsmath,amsfonts,amssymb}
\addtokomafont{labelinglabel}{\sffamily}

\usepackage[boxed]{algorithm2e}
\addtokomafont{labelinglabel}{\sffamily}

% Feynman slash
\usepackage{slashed}

% To show code
\usepackage{listings}

\setlength{\heavyrulewidth}{1.5pt}
\setlength{\abovetopsep}{4pt}

\begin{document}


\title{The dings that will become a thesis at some point}
\author{Ingrid Holm}
\date{February 2018}

\maketitle

\begin{abstract}
This is an abstract text.
\end{abstract}

\begin{dedication}
  To someone
  \\\vspace{12pt}
  This is a dedication to my cat.
\end{dedication}

\begin{acknowledgements}
  I acknowledge my acknowledgements.
\end{acknowledgements}

\tableofcontents


\chapter{Introduction}

- why nlo?

- why gp?

- why dgp?


\chapter{Physics Background}

\section{The Standard Model}

We assume in this thesis that the reader is familiar with the Standard Model of particle physics. Even so, a small introduction is in order. The Standard Model is the best-working framework we have for calculating the behaviour of very small particles at very high energies. At these small levels an important distinction we make is between \textit{fermions} and \textit{bosons}: particles with half-integer and integer spin values, respectively. Fermions interact through the exchange of bosons, and the Standard Model bosons are the \textit{photon} (electromagnetic interaction), the \textit{gluon} (strong interaction that holds atoms together), the $W$ and $Z$ bosons (the weak interaction) and the famously elusive \textit{Higgs boson}. The rules of motion and interaction can all be found using the \textit{Lagrangian} of the Standard Model. A Lagrangian is a measure that is invariant to transformations under the Lorentz group -- or a change of reference frame as it is called in special relativity. The Lagrangian of the SM is given by
\begin{align}
\mathcal{L} = - \frac{1}{4} F_{\mu \nu} F^{\mu \nu} + i \bar{\psi} \slashed{D} \psi + h.c. + x_i y_{ij} x_j \phi + h.c. + |D \phi|^2 - V(\phi),
\end{align}
where $F_{\mu \nu}$ is the electromagnetic field strength, $\psi$ is the fermion field, $\slashed{D}$ is the covariant derivative, $x_iy_{ij}x_j \phi$ are the mass terms, and $|D \phi|^2 - V(\phi)$ is the kinetic term and potential for the Higgs. 


%\subsection{Spontaneous Symmetry Breaking}

%\subsection{The Higgs Mechanism}

\section{Supersymmetry}

Supersymmetry is essentially the extension of the SM, which assumes the SM groups $SU(3) \times SU(2) \times U(1)$ is a subgroup of the superalgebra. This algebra is a coset space of the super-Poincar\'{e} algebra, and the Lorentz algebra. The coordinates in this space are $z^{\pi} = (x^{\mu}, \theta^A, \bar{\theta}_{\dot{A}})$, where $x^{\mu}$ are the well known Minkowski coordinates, and $\theta^A, \bar{\theta}_{\dot{A}}$ are Grassmann numbers that act as parameters of the operators $Q$. A general element of the superalgebra can be written as 
\begin{align}
g = \exp [- ix^{\mu} P_{\mu} + i \theta^A Q_A + i \bar{\theta}_{\dot{A}} \bar{Q}^{\dot{A}} - i \omega_{\rho \nu} M^{\rho \nu}],
\end{align}
where $x^{\mu}$, $\theta^A$, $\bar{\theta}_{\dot{A}}$ and $\omega_{\rho \nu}$ are the parameters of the group, and $P_{\mu}$, $Q_A$, $\bar{Q}^{\dot{A}}$ and $M^{\rho \nu}$ are the group generators. Because of commutation relations and parameterisations (REFERER TIL NOE HER - ARE?) we can write a supersymmetric transformation simply as 
\begin{align}
\delta_S = \alpha^A Q_A + \bar{\alpha}_{\dot{A}} \bar{Q}^{\dot{A}}.
\end{align}
This algebra introduces a symmetry between fermions and bosons, which is brought in by the group generators $Q$ which turn a fermion into a boson and vice versa. These generators can be written as 
\begin{align}
P_{\mu} &= i \partial_{\mu},\\
iQ_A &= -i (\sigma^{\mu} \bar{\theta})_A \partial_{\mu} + \partial_A,\\
i \bar{Q}^{\dot{A}} &= -i (\bar{\sigma}^{\mu} \theta)^{\dot{A}} \partial_{\mu} + \partial^{\dot{A}}.
\end{align}
On our way to describing a supersymmetric Lagrangian we need a derivative that is invariant under supersymmetry transformations. The general covariant derivatives are defined as
\begin{align}
D_A & \equiv \partial_A + i (\sigma^{\mu} \bar{\theta})_A \partial_{\mu},\\
\bar{D}^{\dot{A}} & \equiv -\partial^{\dot{A}} - i (\sigma^{\mu} \theta)^{\dot{A}} \partial_{\mu}.
\end{align}
The next step is to find something for these derivatives to work \textit{on}. For this we define the \textit{superfields} $\Phi(x, \theta, \bar{\theta})$. These can be expanded in power series of $\theta$, $\bar{\theta}$ and $x$, but we won't do that here. We are more concerned with the following relations
\begin{align}
\bar{D}_{\dot{A}} \Phi (x, \theta, \bar{\theta}) &= 0 &\text{(left-handed scalar superfield)},\\
D^A \Phi^{\dagger} (x, \theta, \bar{\theta}) &= 0 &\text{(right-handed scalar superfield)}\\
\Phi (x, \theta, \bar{\theta}) &= \Phi^{\dagger} (x, \theta, \bar{\theta}) &\text{(vector superfield)}.\label{Eq:: vector field}
\end{align}
It can be shown that the left- and right-handed scalar fields can be written as, respectively,
\begin{align}
\Phi (x, \theta, \bar{\theta}) =& A(x) + i (\theta \sigma^{\mu} \bar{\theta}) \partial_{\mu} A(x) - \frac{1}{4} \theta \theta \bar{\theta} \bar{\theta} \Box A(x) + \sqrt{2} \theta \psi (x)\\
& - \frac{i}{\sqrt{2}} \theta \theta \partial_{\mu} \psi (x) \sigma^{\mu} \bar{\theta} + \theta \theta F(x),\\
\Phi^{\dagger} (x, \theta, \bar{\theta}) =& A^*(x) - i (\theta \sigma^{\mu} \bar{\theta}) \partial_{\mu} A^*(x) - \frac{1}{4} \theta \theta \bar{\theta} \bar{\theta} \Box A^*(x) + \sqrt{2} \bar{\theta} \bar{\Psi} (x)\\
& - \frac{i}{\sqrt{2}} \bar{\theta} \bar{\theta} \theta \sigma^{\mu} \partial_{\mu} \bar{\Psi} (x)  + \bar{\theta} \bar{\theta} F^*(x).
\end{align}
From Eq. (\ref{Eq:: vector field}) we see that the structure of a general vector field should be
\begin{align}
\Psi (x, \theta, \bar{\theta}) =& f(x) + \theta \varphi (x) + \bar{\theta} \bar{\varphi} (x) + \theta \theta m(x) + \bar{\theta} \bar{\theta} m^* (x)\\
&+ \theta \sigma^{\mu} \bar{\theta} V_{\mu} (x) + \theta \theta \bar{\theta} \bar{\lambda} (x) + \bar{\theta} \bar{\theta} \theta \lambda (x) + \theta \theta \bar{\theta} \bar{\theta} d(x),
\end{align}
where $f(x)$, $d(x)$ are real scalar fields, $\varphi (x)$, $\lambda (x)$ are Weyl spinors, $m(x)$ is a complex scalar field and $V_{\mu} (x)$ is a real Lorentz four-vector.

Because of various restrictions on the supersymmetric Lagrangian (such as renormalizability), the most general Lagrangian we may write as a function of the scalar superfields is
\begin{align}
\mathcal{L} = \Phi_i^{\dagger} \Phi_i + \bar{\theta} \bar{\theta} W[\Phi] + \theta \theta W[\Phi^{\dagger}],
\end{align}
where $\Phi_i^{\dagger} \Phi_i$ is the kinetic term, and $W[\Phi]$ is the superpotential
\begin{align}\label{Eq:: superpotential}
W[\Phi] = g_i \Phi_i + m_{ij} \Phi_i \Phi_j + \lambda_{ijk} \Phi_i \Phi_j \Phi_k,
\end{align}

Supersymmetry algebra requires that there is an equal number of fermions and bosons, and that these appear in pairs: the `old' SM particles, and their sparticles. An unbroken supersymmetry would mean that the particle-sparticle pairs should have the same mass. Since these particles have not been observed, Supersymmetry is assumed to be a broken symmetry.

The simplest way of breaking SUSY is the \textit{soft breaking}. This entails adding terms to the Lagrangian which cause spontaneous symmetry breaking. Initially, this introduces 104 new parameters (in addition to the existing SM ones). Luckily, various bounds, like the ones from gauge symmetry, reduce the number of parameters significantly. 

The Supersymmetric Lagrangian results in couplings that violate both lepton and baryon numbers. Therefore, we introduce a new, multiplicative conserved quantity, named R-parity
\begin{align}\label{Eq:: R-parity}
R = (-1)^{2s + 3B + L}.
\end{align}
This quantity is $+1$ for SM particles, and $-1$ for the sparticles. Since R-parity must be conserved, this means that sparticles always appear in pairs. A further consequence of this is that there must exist a stable, lightest supersymmetric particle, to which all other supersymmetric particles decay eventually. This is a good candidate for dark matter \cite{weinberg_1995}.


\subsection{Why Supersymmetry?}

\subsubsection{The Hierarchy Problem}

\subsubsection{Gauge Coupling Unification}

\subsubsection{Dark Matter}

\subsection{Superfields}

\subsubsection{Covariant Derivatives}

\subsection{Supersymmetric Lagrangian}

\subsection{Superpartners}

\subsection{R-Parity}

\subsection{The Minimal Supersymmetric Standard Model (MSSM)}

\subsubsection{Lagrangian}

\subsubsection{MSSM Field Content}




\chapter{Supersymmetry at Hadron Colliders}


\subsection{Phenomenology}

\subsection{Current Supersymmetric Limits}

\section{Hadronic Cross Sections}

\subsection{Partonic cross sections}

\subsubsection{Parton Distribution Functions}

\section{Squark-Squark Cross Section}

\subsection{Feynman Diagrams}

\subsection{Calculation to LO}

\subsection{NLO Corrections}

\subsubsection{Loop diagrams}

\subsubsection{Renormalization of Divergences}

\subsubsection{K-factor}

\subsection{State-of-the-art Tools}

Prospino

NLL-fast


\chapter{Gaussian Processes}

\section{Introduction to Bayesian Statistics}

In statistics we distinguish between Bayesian and frequentist statistics, in this thesis we will focus on the former. Bayesian statistics may be called the science of qualified guesses. It's basic principles can be derived from the familiar rules of probability
\begin{align}\label{Eq:: Sum rule}
P(X | I) + P(\bar{X} | I) = 1,
\end{align}
\begin{align}\label{Eq:: Product rule}
P(X, Y | I) = P(X | Y, I) \times P(Y | I),
\end{align} 
commonly known as the \textit{sum rule} and \textit{product rule}, respectively. From these simple expressions we can derive the most central theorem's of Bayesian statistics: namely \textit{Bayes theorem} and \textit{marginalization}, given by
\begin{align}\label{Eq:: Bayes theorem}
P(X | Y, I) = \frac{P(Y | X, I) \times P(X | I)}{P(Y | I)},
\end{align}
\begin{align}
P(X | I) = \int_{- \infty}^{\infty} P(X, Y | I) d Y.
\end{align}

This may seem like an obvious statement: theorem \ref{Eq:: Bayes theorem} is just a way of rephrasing that the probability of $X$ and $Y$ must be the same as the probability of $Y$ and $X$. However, if we choose $X$ and $Y$ more carefully, magic happens. Assume that $X$ is some prediction we have about a , and $Y$ is data 

\subsection{Priors and Likelihood}

Likelihood: probability of the observations given the parameters.

\begin{align}
\text{posterior} = \frac{\text{prior} \times \text{likelihood}}{\text{marginal likelihood}}.
\end{align}

Prior: prior belief or assumption about data. Is modified through likelihood function. Example of coin toss from Sivia.

\begin{figure}
\centering
\includegraphics[scale=0.4]{priors_sivia.png}
\caption{From \cite{sivia2006data}.}
\end{figure}

Posterior: probability of value of a parameter given data and relevant background information.

Likelihood: Probability of parameter given observation.

\subsection{Best Estimate and Reliability}

Best estimate $X_0$ is at maximum of posterior
\begin{align}
&\frac{dP}{dX}\Big|_{X_0} = 0, &\frac{d^2P}{dX^2}\Big|_{X_0} < 0.
\end{align}
How reliable is this best estimate? Find width using Taylor, and take log
\begin{align}
&L = L(X_0) + \frac{1}{2} \frac{d^2}{dx^2} L\Big|_{X_0} (X-X_0)^2 +... ,&L = \log_e \Big[\text{prob}(x | \{data\}, I) \Big]
\end{align}
Proximate posterior with \textbf{Gaussian distribution}
\begin{align}
&\text{prob}(x| \mu, \sigma), &\sigma = \Big( - \frac{d^2L}{dx^2} \Big)^{-1/2}
\end{align}

\begin{figure}
\centering
\includegraphics[scale=0.2]{gaussian_posterior_sivia.png}
\caption{From \cite{sivia2006data}.}
\end{figure}

\subsection{Covariance}

Is the reliability for several parameters $\{ X_i \}$. 
\begin{align}
\frac{dP}{dX_i} \Big|_{X_{0j}} =0,
\end{align}
In 2 dim
\begin{align}
L =& L(X_0, Y_0) + \frac{1}{2} \Big[ \frac{d^2L}{dX^2}  \Big|_{X_0, Y_0}(X-X_0)^2\\
& + \frac{d^2L}{dY^2}  \Big|_{X_0, Y_0}(Y-Y_0)^2 + 2 \frac{d^2L}{dXdY}  \Big|_{X_0, Y_0}(X-X_0)(Y-Y_0) \Big] +...
\end{align}
\begin{align}
Q = 
\begin{pmatrix}
X-X_0 & Y -Y_0
\end{pmatrix}
\begin{pmatrix}
A & C\\
C & B
\end{pmatrix}
\begin{pmatrix}
X -X_0\\
Y-Y_0
\end{pmatrix}
\end{align}
$Q$ is the \textbf{covariance matrix}.

\begin{figure}
\centering
\includegraphics[scale=0.3]{covariances_sivia.png}
\caption{From \cite{sivia2006data}}
\end{figure}


\section{Gaussian Process Regression}

Define mean and covariance function as
\begin{align}
m(\textbf{x}) = \mathbb{E}[f(\textbf{x})]
\end{align}
\begin{align}
k(\textbf{x}, \textbf{x}') = \mathbb{E} [(f(\textbf{x}) - m(\textbf{x}))(f(\textbf{x}') - m(\textbf{x}'))].
\end{align}
Write this as 
\begin{align}
f(\textbf{x}) \sim \mathcal{GP}(m(\textbf{x}), k(\textbf{x}, \textbf{x}'))
\end{align}
Joint distribution for NOISE FREE, train $\textbf{f}$, test $\textbf{f}_*$
\begin{align}
\begin{bmatrix}
\textbf{f}\\
\textbf{f}_*
\end{bmatrix}
\sim 
\mathcal{N} \Bigg(
\boldsymbol{0},
\begin{bmatrix}
K(X, X) & K(X, X_*)\\
K(X, X_*) & K(X_*, X_*)
\end{bmatrix}
 \Bigg)
\end{align}
Then \textbf{condition} distribution on observations
\begin{align}
\textbf{f}_* \big| X_*, X, \textbf{f} \sim \mathcal{N}(K(X_*, X)K(X, X)^{-1} \textbf{f}, K(X_*, X_*) - K(X_*, X)K(X, X)^{-1}K(X, X_*))
\end{align}
Can draw samples from this ditribution.

\begin{figure}
\centering
\includegraphics[scale=0.4]{/home/ingrid/Documents/Master/Programs/Draw_Samples/draw_samples_benchmark.pdf}
\caption{From scikitlearn}
\end{figure}


\subsection{Gaussian Noise Model}

Assume
\begin{align}
&y = f(\textbf{x}) + \varepsilon, &\varepsilon \sim \mathcal{N}(0, \sigma_n^2)
\end{align}
\begin{align}
&\text{cov}(y_p, y_q) = k(\textbf{x}_p, \textbf{x}_q) + \sigma_n^2 \delta_{pq} &\text{cov}(\textbf{y}) = K(X, X) + \sigma_n^2 \mathbb{I}
\end{align}
Distribution now becomes
\begin{align}
\begin{bmatrix}
\textbf{y}\\
\textbf{f}_*
\end{bmatrix}
\sim 
\mathcal{N} \Bigg(
\boldsymbol{0},
\begin{bmatrix}
K(X, X) + \sigma_n^2 \mathbb{I} & K(X, X_*)\\
K(X, X_*) & K(X_*, X_*)
\end{bmatrix}
 \Bigg)
\end{align}
Conditioned
\begin{align}
\textbf{f}_* \big| X_*, X, \textbf{f} &\sim \mathcal{N}(\bar{\textbf{f}}_*, \text{cov}(\textbf{f}_*)),\\
\bar{\textbf{f}}_* &= K(X_*, X) [K(X, X) + \sigma_n^2 \mathbb{I}]^{-1} \textbf{y},\\
\text{cov} (\textbf{f}_*) &= K(X_*, X_*) - K(X_*, X)[K(X, X) + \sigma_n^2 \mathbb{I}]^{-1} K(X, X_*)
\end{align}

GP prediction
\begin{align}
\bar{f}_* &= \textbf{k}_*^T(K + \sigma_n^2\mathbb{I})^{-1} \textbf{y},\label{1}\\
\mathbb{V}[f_*] &= k(\textbf{x}_*, \textbf{x}_*) - \textbf{k}_*^T(K + \sigma_n^2 \mathbb{I})^{-1} \textbf{k}_*.
\end{align}

\subsection{Algorithm}

\begin{algorithm}
\KwData{$X$ (inputs), \textbf{y} (targets), $k$ (covariance function/kernel), $\sigma_n^2$ (noise level), $\textbf{x}_*$ (test input).}
L = Cholesky decomposition ($K + \sigma_n^2 I$) \;
$\boldsymbol{\alpha} = (L^T)^{-1}(L^{-1} \textbf{y})$ \;
$\bar{f}_* = \textbf{k}_*^T \boldsymbol{\alpha}$ \;
$\textbf{v} = L^{-1} \textbf{k}_*$ \;
$\mathbb{V}[f_*] = k(\textbf{x}_*, \textbf{x}_*) - \textbf{v}^T \textbf{v}$ \;
$\log p(\textbf{y}|X) = - \frac{1}{2} \textbf{y}^T \boldsymbol{\alpha} - \sum_i \log L_{ii} - \frac{n}{2} \log 2 \pi$ \;
\KwResult{$f_*$ (mean), $\mathbb{V}[f_*]$ (variance), $\log p(\textbf{y}|X)$ (log marginal likelihood).}
\caption{Algorithm 2.1 from \cite{rasmussen2006gaussian}.}
\label{Alg:: GP}
\end{algorithm}

\section{Covariance Functions}
A function that only depends on the difference between two points, $\textbf{x} - \textbf{x}'$, is called \textit{stationary}. This implies that the function is invariant to translations in input space. If, in addition, it only depends on the length $r=|\textbf{x}-\textbf{x}'|$, the function is \textit{isotropic} (invariant to rigid rotations in input space).  Isotropic functions are commonly referred to as \textit{radial basis functions} (RBFs). The covariance function can also depend on the dot product, $\textbf{x} \cdot \textbf{x}'$, and is then called a \textit{dot product} covariance function.

A function which maps two arguments $\textbf{x} \in \mathcal{X}$, $\textbf{x}' \in \mathcal{X}$ into $\mathbb{R}$ is generally called a \textit{kernel} $k$. Covariance functions are symmetric kernels, meaning that $k(\textbf{x}, \textbf{x}') = k(\textbf{x}', \textbf{x})$. The matrix containing all the covariance elements is called the \textit{covariance matrix}, or the Gram matrix $K$, whose elements are given by
\begin{align}\label{Eq:: covariance matrix}
K_{ij} = k(\textbf{x}_i, \textbf{x}_j).
\end{align}

There are some restrictions on the covariance matrix, namely that is has to be \textit{positive semidefinite} (PSD). This means that the $n \times n $ matrix $K$ satisfies $Q(\textbf{v}) = \textbf{v}^T K \textbf{v} \geq 0 $ for all $\textbf{v} \in \mathbb{R}^n$. A kernel $k$ is PSD if
\begin{align}\label{Eq:: PSD kernel}
\int k(\textbf{x}, \textbf{x}') f(\textbf{x}) f(\textbf{x}') d \mu (\textbf{x}) d \mu (\textbf{x}') \geq 0,
\end{align}
for all $f \in L_2(\mathcal{X}, \mu)$.

\subsubsection{Mean Square Continuity and Differentiability}

Let $\textbf{x}_1, \textbf{x}_2,...$ be a series of points, and $\textbf{x}^*$ be a point in $\mathbb{R}^D$ such that $|\textbf{x}_k - \textbf{x}^*| \rightarrow 0$ as $k \rightarrow \infty$. The condition for a process $f(\textbf{x})$ to be mean square continuous at $\textbf{x}^*$ is then
\begin{align}
\mathbb{E}[|f(\textbf{x}_k)-f(\textbf{x}^*)|^2] \rightarrow 0 \text{ as } k \rightarrow \infty.
\end{align} 
A random field is continuous in mean square if and only if its covariance function $k(\textbf{x}, \textbf{x}')$ is continuous at the point $\textbf{x} = \textbf{x}' = \textbf{x}^*$. This reduces to $k(\boldsymbol{0})$ for stationary covariance functions.

The mean square derivative of $f(\textbf{x})$ in the $i$th direction is given by
\begin{align}
\frac{\partial f (\textbf{x})}{\partial x_i} = \text{l.i.m.}_{h \rightarrow 0} \frac{f(\textbf{x} + h \textbf{e}_i) - f(\textbf{x})}{h},
\end{align}
where l.i.m. denotes the limit in mean square and $\textbf{e}_i$ is the unit vector in the $i$th direction.

\subsection{The Radial Basis Function (RBF)}

The \textit{squared exponential covariance function} (SE) has the form 
\begin{align}
k_{SE} (r) = \exp \Big( - \frac{r^2}{2 \ell^2} \Big),
\end{align} 
where $\ell$ is the \textit{characteristic length scale}. The SE is infinitely differentiable, and so is very smooth. 

Called 
\begin{lstlisting}
from sklearn.gaussian_process.kernels import RBF
rbf = RBF(length_scale=10, length_scale_bounds=(1e-2, 1e2))
\end{lstlisting}



\subsection{The Mat\'{e}rn Kernel}

The \textit{Mat\'{e}rn class of covariance functions} is given by
\begin{align}
k_{Mat\acute{e}rn} (r) = \frac{2^{1- \nu}}{\Gamma (\nu)} \Big( \frac{\sqrt{2 \nu} r	}{\ell} \Big)^{\nu} K_{\nu} \Big( \frac{\sqrt{2 \nu}r}{\ell} \Big),
\end{align}
where $\nu, \ell > 0$, and $K_{\nu}$ is a modified Bessel function. For $\nu \rightarrow \infty$ this becomes the SE. In the case of $\nu$ being half integer, $\nu = p + \frac{1}{2}$, the covariance function is simply the product of an exponential and a polynomial
\begin{align}
k_{\nu=p+\frac{1}{2}} = \exp \Big(- \frac{\sqrt{2 \nu} r	}{\ell} \Big) \frac{\Gamma(p+1)}{\Gamma(2p + 1)} \sum^p_{i=0} \frac{(p+i)!}{i!(p-i)!} \Big( \frac{\sqrt{8 \nu} r	}{\ell} \Big)^{p-i}.
\end{align}
In machine learning the two most common cases are for $\nu = 3/2$ and $\nu = 5/2$
\begin{align}
k_{\nu = 3/2}(r) &=  \Big(1 + \frac{\sqrt{3}r}{\ell} \Big) \exp \Big( -\frac{\sqrt{3}r}{\ell} \Big),\\
k_{\nu = 5/2}(r) &=  \Big(1 + \frac{\sqrt{5}r}{\ell}  + \frac{5r^2}{3 \ell^2}\Big) \exp \Big( -\frac{\sqrt{5}r}{\ell} \Big).
\end{align}

\begin{lstlisting}
from sklearn.gaussian_process.kernels import Matern
matern = Matern(length_scale=10, length_scale_bounds=(1e-2, 1e2), nu=1.5)
\end{lstlisting}

\subsubsection{Other Kernels}

There are other kernels, but they are not used here. Can me multiplied and summed. For more info check chapter 4 in \cite{rasmussen2006gaussian}.





\subsection{Hyperparameters}

Each kernel has a vector of hyperparameters, e.g. $\boldsymbol{\theta} = (\{M\}, \sigma^2_f, \sigma_n^2)$ for the radial basis function (RBF)
\begin{align}
k(\textbf{x}_p, \textbf{x}_q) = \sigma_f^2 \exp (- \frac{1}{2} (\textbf{x}_p - \textbf{x}_q))^T M (\textbf{x}_p - \textbf{x}_q) + \sigma_n^2 \delta_{pq}.
\end{align}
The matrix $M$ can have several forms, for example
\begin{align}
M_1 = \ell^{-2} \mathbb{I} , M_2 = \text{diag}(\vec{\ell})^{-2}.
\end{align}



\section{Model Selection}



\subsection{Bayesian Model Selection}

Feature selection at several levels: posterior over \textit{parameters}, posterior over \textit{hyperparameters} and posterior for the \textit{model},
\begin{align}
p(\textbf{w}| \textbf{y}, X, \boldsymbol{\theta}, \mathcal{H}_i) = \frac{p(\textbf{y} | X, \textbf{w}, \mathcal{H}_i) p(\textbf{w}|\boldsymbol{\theta}, \mathcal{H}_i)}{p(\textbf{y}|X, \boldsymbol{\theta}, \mathcal{H}_i)}
\end{align}
\begin{align}
&p(\textbf{y}|X, \boldsymbol{\theta}, \mathcal{H}_i) = \int p(\textbf{y} | X, \textbf{w}, \mathcal{H}_i)p(\textbf{w}| \boldsymbol{\theta}, \mathcal{H}_i) d \textbf{w} & \text{(marginal likelihood)}
\end{align}
\begin{align}
p( \boldsymbol{\theta}| \textbf{y}, X, \mathcal{H}_i) = \frac{p(\textbf{y} | X, \boldsymbol{\theta}, \mathcal{H}_i) p(\boldsymbol{\theta}| \mathcal{H}_i)}{p(\textbf{y}|X,  \mathcal{H}_i)}
\end{align}
\begin{align}
p(\mathcal{H}_i| \textbf{y}, X) = \frac{p(\textbf{y} | X, \mathcal{H}_i) p( \mathcal{H}_i)}{p(\textbf{y}|X)}
\end{align}

\subsection{Log Marginal Likelihood}

For Gaussian Processes with Gaussian we can find the exact expression for the marginal likelihood,
\begin{align}
\log p(\textbf{y}|X, \boldsymbol{\theta}) = - \frac{1}{2} \textbf{y}^T K_y^{-1} \textbf{y} - \frac{1}{2} \log |K_y| - \frac{n}{2} \log 2 \pi.
\end{align}
The optimal parameters are found by maximizing the marginal likelihood
\begin{align}
\frac{\partial}{\partial \theta_j}
 \log p(\textbf{y}|X, \boldsymbol{\theta}) = \frac{1}{2} \textbf{y}^T K^{-1} \frac{\partial K}{\partial \theta_j} K^{-1} \textbf{y} - \frac{1}{2} \text{tr} (K^{-1} \frac{\partial K}{\partial \theta_j}).
\end{align}
This is what SciKitLearn uses, but can have \textbf{multiple local maxima}. Plug in Fig. 

\begin{figure}
\centering
\includegraphics[scale=0.6]{/home/ingrid/Documents/Master/Programs/LML/plots/LML_two_local_maxima.pdf}
\caption{Used SciKitLearn. Several local maxima.}
\end{figure}


\subsection{Cross Validation}

Divide into $k$-subsets and use validation and test set. Requires a loss function, e.g. $R^2$. Cross validate using Scikit-Learn, get a validation plot. Over training, over fitting, under-fitting.

\begin{figure}
\centering
\includegraphics[scale=0.4]{/home/ingrid/Documents/Master/ML/Benchmarks/learningcurve.pdf}
\caption{From scikit-learn.}
\end{figure}

\begin{figure}
\centering
\includegraphics[scale=0.4]{/home/ingrid/Documents/Master/ML/Benchmarks/learningcurve_2.pdf}
\caption{From scikit-learn.}
\end{figure}




\subsection{Loss functions}

\subsubsection{Mean Relative Deviance}

The mean relative deviance is used to quantify the quality of predictions
\begin{align}
\varepsilon = \frac{y_{true} - y_{GP}}{y_{true}}
\end{align}

\subsubsection{R-Factor}


\section{Distributed Gaussian Processes}

\subsection{Limitations of Gaussian Processes}

Problem because of $(K + \sigma_n^2 \mathbb{I})^{-1}$, means inverting an $n \times n$-matrix. Training and predicting limits of $\mathcal{O}(N^3)$ and $\mathcal{O}(N^2)$. Limit = $\mathcal{O}(10^4)$. Some solutions exist, but \textbf{no prediction of variance is given with p-o-e.}

\subsection{Product-of-Experts}

Divide data between experts. "The assumption of independent GP experts leads to a
block-diagonal approximation of the kernel matrix, which
(i) allows for efficient training and predicting (ii) can be
computed efficiently (time and memory) by parallelisation" \cite{deisenroth2015distributed}.

\begin{figure}
\centering
\includegraphics[scale=0.3]{product_of_experts.png}
\caption{From \cite{deisenroth2015distributed}.}
\end{figure}

Independence assumption
\begin{align}
p(\textbf{y} | \textbf{X}, \boldsymbol{\theta}) \approx \prod_{k=1}^M p_k(\textbf{y}^{(k)} | \textbf{X}^{(k)}, \boldsymbol{\theta}) 
\end{align}
\begin{align}
\log p(\textbf{y}^{(k)}|\textbf{X}^{(k)}, \boldsymbol{\theta}) = - \frac{1}{2} \textbf{y}^{(k)} (\textbf{K}_{\psi}^{(k)} + \sigma_{\varepsilon}^2 \textbf{I})^{-1}\textbf{y}^{(k)} - \frac{1}{2} \log
 |\textbf{K}_{\psi}^{(k)} + \sigma_{\varepsilon}^2 \textbf{I} |
\end{align}


\subsection{Algorithm}

\begin{align}
\mu_*^{rbcm} &= (\sigma_*^{rbcm})^2 \sum_k \beta_k \sigma_k^{-2} (\textbf{x}_*) \mu_k (\textbf{x}_*),\\
(\sigma_*^{rbcm})^{-2} &= \sum_{k=1}^M \beta_k \sigma_k^{-2} (\textbf{x}_*) + \big(1 - \sum_{k=1}^M \beta_k \big) \sigma_{**}^{-2}.
\end{align}
The posterior distribution for the test point $\textbf{x}_*$ is given by a Gaussian with mean and variance
\begin{align}
\mu (\textbf{x}_*) &= \textbf{k}_*^T (\textbf{K} + \sigma_{\epsilon}^2 \mathbb{I})^{-1} \textbf{y},\\
\sigma^2(\textbf{x}_*) &= k_{**} - \textbf{k}_*^T(\textbf{K} + \sigma_{\epsilon}^2 \mathbb{I})^{-1} \textbf{k}_*.
\end{align}




Plot of time from the article 

\begin{figure}
\centering
\includegraphics[scale=0.4]{DGP_times.png}
\caption{From \cite{deisenroth2015distributed}.}
\end{figure}

\subsection{Implementing the Algorithm}

\begin{algorithm}
 \KwData{$N_{experts}$ (number of experts), $X$ (inputs), \textbf{y} (targets), $k$ (covariance function/kernel), $\sigma_n^2$ (noise level), $\textbf{x}^*$ (test input), $\textbf{y}^*$ (test target)}
$X_{train}$, $X_{test}$, $y_{train}$, $y_{test}$ = train-test-split $(X, y)$ (scikit-learn) \;
$y = \log_{10} (y)$ \;
$n = \frac{\text{Number of data points}}{N_{experts}}$ \;
$subsets = array\_split (X_{train}, n)$ \;
$\mu_{rbcm} = []$, $\sigma_{rbcm} = []$  (empty lists to be filled later)\; 
\For {each expert}
{
$gp_{temporary} = GaussianProcessRegressor.fit(X_{expert}, y_{expert})$ \;
 \For {each $y^*$ in $\textbf{y}^*$}
 {
 $\mu_*,\sigma_*^2 = gp_{temporary}.predict(x^*)$ \;
 $\sigma_{**}^2 = k (x^*, x^*)$ \;
 (fill inn the values) \;
 $\boldsymbol{\mu}[\text{expert}][x^*] = \mu_*^2$ (mean value from this expert)\;
 $\boldsymbol{\sigma}^2[\text{expert}][x^*] = \sigma_*^2$ (variance from this expert)\;
 $\boldsymbol{\sigma}_{**}^2[\text{expert}][x^*] = \sigma_{**}^2$ (variance from initial kernel)
 }
}
 
\For {each expert}
{ 
\For {each $y_*$ in $\textbf{y}_*$}
{ $\mu_* = \boldsymbol{\mu}[\text{expert}][x_*]$ (retrieve relevant values)\;
$\sigma_*^2 = \boldsymbol{\sigma}^2[\text{expert}][x^*]$ \;
$\sigma_{**}^2 = \boldsymbol{\sigma}_{**}^2[\text{expert}][x^*]$ \; 
$\beta = \frac{1}{2} (\log (\sigma_{**}^2) - \log (\sigma_*^2))$ \;
$(\sigma_*^{rbcm})^{-2}[y_*] += \beta \sigma^{-2} + \big(\frac{1}{n_{experts}} - \beta \big) \sigma_{**}^{-2} $ }
 }  
\For {each expert}
{
\For {each $y_*$ in $\textbf{y}_*$}
 {
$\mu_* = \boldsymbol{\mu}[\text{expert}][x_*]$ (retrieve relevant values)\;
$\sigma_*^2 = \boldsymbol{\sigma}^2[\text{expert}][x^*]$ \;
$\sigma_{**}^2 = \boldsymbol{\sigma}_{**}^2[\text{expert}][x^*]$ \; 
$\beta = \frac{1}{2} (\log (\sigma_{**}^2) - \log (\sigma_*^2))$ \;
$\mu_*^{rbcm}[y_*] += (\sigma_*^{rbcm})^2 \beta \sigma^{-2}_* \mu_*$
 }
} 
$\epsilon = \frac{10^{\mu_{rbcm}} - 10^{y_{test}}}{10^{y_{test}}}$ (relative error)\;
\KwResult{Approximative distribution of $f_* = f(\textbf{x}_*)$ with mean $\mu^{rbcm}_*$ and variance $(\sigma^{rbcm}_*)^2$.}
 \caption{Algorithm for using rBCM on a single test point $\textbf{x}_*$. The $GaussianProcessRegressor.fit()$-function is a function in scikit-learn, that uses Algorithm (\ref{Alg:: GP}). }
\label{Alg:: DGP}
\end{algorithm}

\subsubsection{Parallelizing}

\subsection{Benchmark}





\chapter{Calculating Cross Sections with Distributed Gaussian Processes}

\section{Data Generation}

How was the data generated?

\subsubsection{Prospino}

- Possible issues

- NLL-FAST

- SOFTSUSY

\subsubsection{Feature Distributions}

Lin and log distributions. 

\subsubsection{Data Quality}

\begin{figure}
\centering
\includegraphics[scale=0.4]{/home/ingrid/Documents/Master/ML/Final_remarks/Matern44_new/feature_dist_lin_mat44m2.pdf}
\end{figure}

\section{Dataset Transformations}

\subsection{Removing Outliers}

\subsubsection{Prospino Sets NLO Cross Sections to 0}

- When all squark masses are large, and larger than $m_{\tilde{c}_L}$, the $K$-factor is zero and $LO \neq 0$ but $NLO = 0$.

The new running of 4 experts with 11 000 points each did not give a very good result. Looking at the plots of $\sigma_{m_{g}}$ versus $m_{\tilde{q}}$ and $m_{\tilde{g}}$ made us notice a few outliers which where all of the order $\log_{10} \sigma = -32$. This we attributed to the program running the Gaussian processes setting all zero-cross sections to equal $10^{-32}$. Looking further into the slha-files we noticed that for these points, all squark masses were very high, but the relevant squark mass $m_{c_L}$ was a little lower than the others. This probably caused the K-factor to be zero, and therefore all NLO cross sections in this file were zero, while all LO cross section were not. We removed these points and saw a large improvement in the prediction for $\sigma_{m_{\tilde{g}}}$, making it much more stable in the low cross sections. The varying quality of the previous runs was attributed to the large dependence of number of outliers of the prediction.

We noticed the outliers when plotting the cross section as a function og the squark-mass, as seen in Fig. (\ref{Fig:: sigma mq true with outliers}). The distributed Gaussian processes are unable to predict these outliers, which come from setting zero-cross sections to $10^{-32}$, as can be seen from Fig. (\ref{Fig:: sigma mq predicted mat44m2_new}). The points seem to make the prediction for small cross sections worse, as this aerea appears to have a lot of noise. 

\begin{figure}
\centering
\includegraphics[scale=0.3]{/home/ingrid/Documents/Master/ML/Final_remarks/edited_sigmamq_true_lin_mat44m2.pdf}
\caption{True values for the logarithm of NLO cross section as a function of $m_{\tilde{c}_L}$, for 20 000 points from the lin set. Outliers are circled in purple.}
\label{Fig:: sigma mq true with outliers}
\end{figure}

\begin{figure}
\centering
\includegraphics[scale=0.3]{/home/ingrid/Documents/Master/ML/Final_remarks/Matern44_new/sigmamq_predicted_lin_mat44m2.pdf}
\caption{Values predicted by Distributed Gaussian processes on the lin set, using 4 experts with 11 000 points each, and the Mat\'{e}rn kernel. There are 20 000 test points, and the outliers from Fig. (\ref{Fig:: sigma mq true with outliers}) are missing.}
\label{Fig:: sigma mq predicted mat44m2_new}
\end{figure}

The problematic points were therefore removed in a new run with 4 experts with 11 000 points each, still using the Mat\'{e}rn kernel. The resulting mean relative deviations can be seen in Fig. (\ref{Fig:: ms mat44m2 zeros nozeros}).

\begin{figure}
\centering
\includegraphics[scale=0.3]{/home/ingrid/Documents/Master/ML/Final_remarks/ms_compare_mat44m2_zero_nozero.pdf}
\caption{Mean standard deviation $\varepsilon = \frac{y_{true} - y_{dgp}}{y_{true}}$ for 4 experts with 11 000 points each from the lin set, with 20 000 test points. The Mat\'{e}rn kernel with $\nu = 1$ was used. The green lines are for before the outliers were removed, and the blue lines are from after.}
\label{Fig:: ms mat44m2 zeros nozeros}
\end{figure}



\subsection{Lower Cuts on Cross sections ($\propto 10^{-16}$)}


\section{Choosing the Kernel}

\subsubsection{Mat\'{e}rn}

\subsubsection{Hyperparameter $\nu$}

\subsubsection{Vectorized $\ell$}


\section{When the Error is Known}

\subsubsection{Proof for Relative Errors}

The error in the observations comes from the numerical error (?) in the Prospino calculation. The relative error has a standard deviation of $\varepsilon = 0.002$ multiplied by the cross section. The question is \textit{whether we can use this information} when doing the GP fit. Denote the cross section provided by Prospino as $Y_i$ and the real cross section as $y_i^{true}$, we then have
\begin{align}\label{Eq:: cross section w/ error}
Y_i = y^{true}_i + \epsilon_i = y_i^{true}(1 + \varepsilon_i).
\end{align}


However, we do not calculate the prediction of the cross section, but the \textit{logarithm} of the cross section. The distributions are then
\begin{align}
Y_i = \mathcal{N}(y_i^{true}, \varepsilon y_i^{true}),
\end{align}
where the only random variable is $\varepsilon$. Changing variables to $\log_{10}$ gives
\begin{align}
X_i &= \log_{10} Y_i \rightarrow Y_i = 10^{X_i}\\
P_{X_i} (X_i) &= P_{Y_i} (Y_i) \Big|\frac{\partial Y_i}{\partial X_i}\Big|\\
&= P_{Y_i} (y_i) 10^{X^i} \log 10\\
&= \mathcal{N} (10^{x_i^{true}}, \varepsilon 10^{x_i^{true}}) \cdot 10^{X_i} \cdot \log 10.
\end{align}
This means that the relevant distribution is in fact
\begin{align*}
X_i = \log_{10} Y_i = \log_{10} y_i^{true} + \log_{10} (1 + \mathcal{N}(0, \varepsilon)),
\end{align*}
where we can make the expansion
\begin{align}
\log_{10} (1 + \mathcal{N}(0, \varepsilon)) \simeq \frac{\mathcal{N} (0, \varepsilon)}{\log 10} - \frac{\mathcal{N} (0, \varepsilon)^2}{\log 100} +...
\end{align}

Since the leading order term is clearly the dominant term, the logarithm of the cross section may be written as
\begin{align}\label{Eq:: cross section log gaussian noise}
X_i \simeq \log_{10} y_i^{true} + \frac{1}{\log 10} \mathcal{N} (0, \varepsilon)
\end{align}

Since the distribution has a standard deviation $\varepsilon = 2 \cdot 10^{-3}$, the Gaussian noise covariance should be
\begin{align*}
\Big( \frac{\varepsilon}{\log 10} \Big)^2 = \frac{(2 \cdot 10^{-3})^2}{(\log 10)^2} = \frac{4 \cdot 10^{-6}}{5.301} \simeq 7.544 \cdot 10^{-7}.
\end{align*}



\section{Features and Target}

\subsection{Lagrangian Masses}

Not good.

\subsection{Physical Masses}

\subsubsection{Adding the Mean Mass}

How Prospino calculates NLO terms.




\chapter{Results}

\section{Final Kernel}

\subsubsection{Mat\'{e}rn}

\subsubsection{Hyperparameters}

- LML plot

\subsubsection{Gaussian Noise $\alpha$}

\subsection{Posterior}

\subsubsection{Drawing Samples}


\section{Mean Relative Deviance}


\section{Learning Curve}

\subsubsection{$d_Ld_L$}

\subsubsection{$d_Lu_L$}

\section{Distributed Gaussian Processes}

\subsubsection{Adding Experts Improves the Prediction}

- Plot of loss function as function of number of experts.

- Time table

\section{Comparison with Existing Methods}

\subsection{NLL-fast}

- Non-degenerate masses and cross sections

\subsection{Prospino}

- Faster



\chapter{Conclusions}

- Distributed Gaussian Processes work: more experts give better results

- Adding the mean was very important

- The method is sensitive to outliers

- Works well with relatively few data points ($4 \times 8000$ points )


\chapter*{Appendix A: The Gaussian Distribution}


\bibliographystyle{plain}
\bibliography{dingsen}

\chapter*{Appendix B: Benchmarks}

\section{When the Error is Known}

Most observations are noisy versions of the true function values. In \cite{rasmussen2006gaussian} there is an example using additive independent identically distributed Gaussian noise $\varepsilon$ with variance $\sigma_n^2$ (and standard deviation $\sigma_n$). The benchmark function attempts to convert the noisy function at hand to the form
\begin{align}\label{Eq:: Gaussian noise}
y = f(x) + \varepsilon \text{ , } \varepsilon \sim  \mathcal{N} (0, \sigma_n).
\end{align}

This may, according to \ref{Eq:: cross section log gaussian noise}, be done. Assume a relatively simple function which spans over several orders of magnitude, with a shape that can easily be modelled by the RBF kernel. The function used here is
\begin{align}
f(x) =  1000 \exp \Big(-\frac{x^2}{10}\Big) \text{ , } x \in [0, 30],
\end{align} 
with a corresponding noisy function 
\begin{align}
y = f(x) + \varepsilon f(x),
\end{align}
where $\varepsilon \sim \mathcal{N} (0, \sigma_n^2)$.

A plot of the true function and noisy function, along with their logarithms, are shown in Fig. (\ref{Fig:: y and fx} -\ref{Fig:: log y and log fx}). A large standard deviation for the error is used for illustrational purposes. The error is relatively much smaller for the log of functions, and more evenly distributed, as can be seen from the zoom in in Fig (\ref{Fig:: log y and log fx zoom}).

\begin{figure}
\centering
\includegraphics[scale=0.3]{/home/ingrid/Documents/Master/ML/Benchmarks/known_error/y_fx_alpha01.pdf}
\caption{The pure and noisy function with $\varepsilon \sim \mathcal{N} (0, \sigma_{std} = 0.1)$.}
\label{Fig:: y and fx}
\end{figure} 

\begin{figure}
\centering
\includegraphics[scale=0.3]{/home/ingrid/Documents/Master/ML/Benchmarks/known_error/y_fx_log_alpha01.pdf}
\caption{The logarithm of the pure and noisy function with $\varepsilon \sim \mathcal{N} (0, \sigma_{std} = 0.1)$.}
\label{Fig:: log y and log fx}
\end{figure} 

\begin{figure}
\centering
\includegraphics[scale=0.3]{/home/ingrid/Documents/Master/ML/Benchmarks/known_error/y_fx_log_alpha01_zoom.pdf}
\caption{The logartihm of the pure and noisy function with $\varepsilon \sim \mathcal{N} (0, \sigma_{std} = 0.1)$.}
\label{Fig:: log y and log fx zoom}
\end{figure} 

For the actual training and prediction, a standard deviation of $\sigma_{std} = 0.001$ was used (to get a similar number to the real case of the cross sections). A single Gaussian process was performed with 500 training points and 4000 test points, using the RBF kernel. The Gaussian process was first tested on the true function, and the error measure was the relative error given by
\begin{align}
\epsilon_{GP} = \frac{y_{true} - y_{predicted}}{y_{true}},
\end{align}
where $y_{predicted} = 10^{\log_{10} f_{predicted}}$, and $f_{predicted}$ is what the GP predicts. The optimal hyperparameters for the RBF kernel on the true function were
\begin{align*}
\text{kernel}_{true} = 3.16^2 \exp \Big( - \frac{x^2}{5.6^2} \Big),
\end{align*}
with no noise. A histogram of the errors is shown in Fig. (\ref{Fig:: fx prediction no noise}).

\begin{figure}
\centering
\includegraphics[scale=0.3]{/home/ingrid/Documents/Master/error_investigation/fx_log_nowk.pdf}
\caption{Error histogram of the Gaussian process prediction for the true function $f(x)$ using a noiseless RBF-kernel.}
\label{Fig:: fx prediction no noise}
\end{figure}

The prediction for the error is
\begin{align*}
\Big( \frac{\varepsilon}{\log 10} \Big)^2 = \frac{(1 \cdot 10^{-3})^2}{(\log 10)^2} = 1.886 \cdot 10^{-7}.
\end{align*}

Setting $\alpha = 1.886 \cdot 10^{-7}$ gives the best prediction, with a kernel with very similar hyperparameters to the noise-free case
\begin{align}
\text{kernel}_{noisy} = 3.16^2 \exp \Big( - \frac{x^2}{5.82^2} \Big).
\end{align}

It also improves the prediction significantly from the case where no noise is fed to the estimator. A plot of the case where $\alpha = 10^{-10}$ and $\alpha = 1.886 \cdot 10^{-7}$ are shown in Fig. (\ref{Fig:: y prediction no noise}) and Fig. (\ref{Fig:: y prediction alpha=1.886}), respectively.

\begin{figure}
\centering
\includegraphics[scale=0.3]{/home/ingrid/Documents/Master/error_investigation/y_log_nowk.pdf}
\caption{Error histogram of the Gaussian process prediction for the noisy function $y(x)$ using a noiseless RBF-kernel ($\alpha = 10^{-10}$).}
\label{Fig:: y prediction no noise}
\end{figure}

\begin{figure}
\centering
\includegraphics[scale=0.3]{/home/ingrid/Documents/Master/error_investigation/y_log_alpha1886min7.pdf}
\caption{Error histogram of the Gaussian process prediction for the noisy function $y(x)$ using an RBF-kernel with $\alpha = 1.886 \cdot 10^{-6}$.}
\label{Fig:: y prediction alpha=1.886}
\end{figure}

The best value for $\alpha$ is in fact very close to our estimate, as can be seen from Fig. (). Note that $\alpha \propto 10^{-8}$ gives a smaller mean than $\alpha \propto 10^{-7}$, but it also has a much larger variance.

\begin{figure}
\centering
\includegraphics[scale=0.3]{/home/ingrid/Documents/Master/error_investigation/y_log_sigma_alpha.pdf}
\caption{Standard deviation of error distribution of the Gaussian process prediction for a varying noise level variance $\alpha$.}
\label{Fig:: y prediction alpha mu}
\end{figure}



\end{document}