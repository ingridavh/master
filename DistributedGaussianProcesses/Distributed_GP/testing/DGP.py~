"""
Implement and test the Distributed Gaussian Processes using
SciKitLearn.

Compares with full GP for small datasets.

@author: Ingrid A V Holm
"""
import numpy as np
import matplotlib.pyplot as plt
import sys
from scipy.stats import norm
import scipy.stats as stats
import matplotlib.mlab as mlab

#Import pandas for data formatting
import pandas as pd

#Import SciKit-learn modules
import sklearn as sk
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, WhiteKernel
from sklearn.cross_validation import train_test_split
from sklearn.datasets import load_boston

#Define a simple function for benchmarking
def f_x(x):
    return  4*x[:,0]*x[:,1] 

np.random.seed(42)
x_values = abs(np.random.randn(10000,2))*10
y_values = f_x(x_values)

# Inputs from command line

if len(sys.argv) >= 2:
    #Set fraction of data to be used as training data
    trainsize = float(sys.argv[1])
else:
    trainsize = 0.1

if len(sys.argv) >= 4:
    #OBS! Fix this and read data from file! Pandas!!!!!!!!!!!!!!!!!!!!!! 
    infile = sys.argv[3]
else:
    #Load toy data set
    #If return_X_y=True this returns (data, target) tuple
    infile = load_boston(return_X_y = True)
    print "No input file was given, so I'll use the toy data set 'boston'!"

if len(sys.argv) >= 3:
    #Read name of outfile from command line
    outfile_name = sys.argv[2]
else:
    outfile_name = "results.txt"

print "I will dump the results in file: ", outfile_name

#Make data ready for Gaussian Process

data = x_values
targets = y_values

print "Input data dimensions: ", data.shape
print "Target shape: ", targets.shape

#Split into training and test data
X_train, X_test, y_train, y_test = train_test_split(data, targets, random_state=42, train_size=trainsize)

# Change to log
y_train = np.log10(y_train)
y_test = np.log10(y_test)

#Choose the number of data subsets
n_subsets = 4

#Choose kernel
kernel = RBF(length_scale=1, length_scale_bounds=(1e-02, 1e05)) + WhiteKernel(noise_level=0.1, noise_level_bounds=(1e-03, 1000.0))


############################################################
# Regular GP                                               #
############################################################

#Fit GP to training data
print "Fitting kernel to data..."
gp = GaussianProcessRegressor(kernel=kernel, alpha=0.001, normalize_y=False).fit(X_train, y_train)
print "Finished fitting!"

#Print mean and std
print "Initial kernel: ", kernel
print "Kernel after fit: ", gp.kernel_

# Find prior kernel
prior_kernel =  kernel(X_test)

#Predict for test data
y_predict, cov_y = gp.predict(X_test, return_cov=True)
y_predict, std_y = gp.predict(X_test, return_std=True)

############################################################
# DGP                                                      #
############################################################

n_points = len(data[:,])
len_subsets = n_points/n_subsets

subsets_X = np.array_split(X_train, n_subsets)
subsets_y = np.array_split(y_train, n_subsets)

kernel_params = []
means = np.zeros((n_subsets, len(X_test)))
variances = []

sigma_rbcm_neg = np.zeros(len(X_test))
mu_rbcm = np.zeros(len(X_test))

mus_experts = np.zeros((n_subsets,len(X_test)))
sigmas_experts = np.zeros((n_subsets,len(X_test)))
prior_covs = np.zeros((n_subsets,len(X_test)))


for i in range(n_subsets):
    print "Expert number %i reporting for duty" % int(i+1)
    print "Fitting kernel to data..."
    gp_temp = GaussianProcessRegressor(kernel=kernel, alpha=0.001, normalize_y = False, n_restarts_optimizer = 0).fit(subsets_X[i], subsets_y[i])
    print "Finished fitting!"
    kernel_params.append(gp_temp.kernel_)
    print "Kernel parameters: ", gp_temp.kernel_

    #Predict y-values using the test set, and save mean and variance
    #y_predict_mean, y_predict_cov = gp_temp.predict(X_test, return_cov=True)
    
    # Do BCM stuff
    for k in range(len(X_test)):
        my_X = X_test[k].reshape(1,-1)
        mu_star, sigma_star_mean = gp_temp.predict(my_X, return_cov=True)
        prior_cov = kernel(X_test[k])

        mus_experts[i][k] = mu_star
        sigmas_experts[i][k] = sigma_star_mean
        prior_covs[i][k] = prior_cov
        

for i in range(n_subsets):
    # Do BCM stuff
    for k in range(len(X_test)):
        mu_star = mus_experts[i][k]
        sigma_star_mean = sigmas_experts[i][k]
        prior_cov = prior_covs[i][k]
        
        beta = 0.5*(np.log(prior_cov)-np.log(sigma_star_mean))
        sigma_rbcm_neg[k] += beta*sigma_star_mean**(-1)+(1./n_subsets - beta)*prior_cov**(-1)


for i in range(n_subsets):
    # Find mus
    for k in range(len(X_test)):
        mu_star = mus_experts[i][k]
        sigma_star_mean = sigmas_experts[i][k]
        prior_cov = prior_covs[i][k]

        beta = 0.5*(np.log(prior_cov)-np.log(sigma_star_mean))
    
        mu_rbcm[k] +=  sigma_rbcm_neg[k]**(-1)*(beta*sigma_star_mean**(-1)*mu_star)
    

rel_err = (10**y_test -10**mu_rbcm)/10**y_test
rel_err_gp = (10**y_test - 10**y_predict)/10**y_test

########################################################
# Write results to file                                #
########################################################

outfile = open(outfile_name, 'w')
outfile.write('Test results: GP Error -- DGP Error -- Mus -- Sigmas \n')
for i in range(len(X_test)):
    outfile.write(str(rel_err_gp[i])+' ')
    outfile.write(str(rel_err[i])+' ')
    outfile.write(str(mu_rbcm[i])+ ' ')
    outfile.write(str(sigma_rbcm_neg[i])+' ')
    outfile.write('\n')

outfile.close()

print "Results are found in file", outfile_name
